import os
import pandas as pd
import glob
from datetime import datetime
import sys
import yfinance as yf
import concurrent.futures # ç”¨äºå¹¶å‘åŠ é€Ÿè·å–ä¿¡æ¯
import json
import threading

# --- è·¯å¾„ä¿®å¤ ---
current_dir = os.path.dirname(os.path.abspath(__file__)) # src/micro
src_dir = os.path.dirname(current_dir)                   # src
project_root = os.path.dirname(src_dir)                  # MY-DOGE-MICRO

if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- å¯¼å…¥ ---
try:
    from src.macro.config import MacroConfig
    from src.macro.strategist import DeepSeekStrategist
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥æ•°æ®åº“ä¿å­˜å‡½æ•°
try:
    from src.micro.database import save_research_report
except ImportError:
    try:
        from database import save_research_report
    except ImportError:
        print("âš ï¸ Warning: Could not import database module")
        save_research_report = lambda *args, **kwargs: None

class IndustryAnalyzer:
    def __init__(self, logger_callback=None, proxy='http://127.0.0.1:7890'):
        self.config = MacroConfig()
        self.strategist = DeepSeekStrategist(self.config)
        self.project_root = project_root
        self.logger_callback = logger_callback
        self.cache_file = os.path.join(self.project_root, 'data', 'meta_cache.json')
        self.cache_lock = threading.RLock()
        self.metadata_cache = self._load_cache()
        # ç”¨äºè®°å½•æœ¬æ¬¡åˆ†æä¸­æ–°è·å–çš„è‚¡ç¥¨ä»£ç 
        self.newly_fetched_tickers = set()
        
        # è®¾ç½®yfinanceä»£ç†ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
        self.proxy = proxy
        os.environ['HTTP_PROXY'] = proxy
        os.environ['HTTPS_PROXY'] = proxy

    def load_latest_file(self, pattern):
        """åŠ è½½æœ€æ–°çš„æ–‡ä»¶"""
        files = glob.glob(pattern)
        if not files:
            return None
        return max(files, key=os.path.getctime)

    def log(self, message):
        """æ—¥å¿—è¾“å‡ºï¼šåŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œå›è°ƒå‡½æ•°"""
        print(message)
        if self.logger_callback:
            self.logger_callback(message)

    def _format_stock_line(self, row, name, sector):
        """ç»Ÿä¸€æ ¼å¼åŒ–è‚¡ç¥¨ä¿¡æ¯è¡Œ"""
        # [NEW] å¢åŠ  RSRS æ•°æ®çš„å±•ç¤º
        # å¦‚æœ csv ä¸­æ²¡æœ‰ rsrs_z åˆ—ï¼Œé»˜è®¤ä¸º 0.0
        rsrs_val = row.get('rsrs_z', 0.0)
        
        # å¢åŠ ä¸€ä¸ªè§†è§‰æ ‡è®°ï¼šRSRS > 0.8 ä¸ºå¼ºè¶‹åŠ¿ (ğŸ”¥)
        trend_mark = "ğŸ”¥" if rsrs_val > 0.8 else ""
        
        return (
            f"- {row['ticker']} [{name}] ({sector}) "
            f"| æ¶¨å¹…: +{row['change_percent']}% "
            f"| RSRS: {rsrs_val} {trend_mark}"
        )

    def _process_csv(self, file_path, market_type):
        """å¤„ç† CSV æ–‡ä»¶å¹¶æ³¨å…¥å…ƒæ•°æ®ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        df = pd.read_csv(file_path)
        # å–å‰ 50 åï¼Œé¿å… Token æº¢å‡ºï¼Œä¸”å¤´éƒ¨æ•ˆåº”æœ€æ˜æ˜¾
        top_50 = df.head(50) 
        
        self.log(f"ğŸ” æ­£åœ¨è”ç½‘æ ¡å‡† {market_type} å‰ 50 åè‚¡ç¥¨çš„ä¸šåŠ¡ä¿¡æ¯...")
        stock_list_str = []
        
        # å¹¶å‘è·å–ï¼Œé¿å…å¡é¡¿
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_ticker = {executor.submit(self.get_stock_metadata, row['ticker']): row for _, row in top_50.iterrows()}
            
            for future in concurrent.futures.as_completed(future_to_ticker):
                row = future_to_ticker[future]
                name, sector = future.result()
                
                # [MODIFIED] è°ƒç”¨ç»Ÿä¸€çš„æ ¼å¼åŒ–å‡½æ•°
                line = self._format_stock_line(row, name, sector)
                stock_list_str.append(line)
        
        return "\n".join(stock_list_str)

    def load_macro_context(self):
        """è¯»å–æœ€æ–°çš„å®è§‚æŠ¥å‘Šæ‘˜è¦"""
        # æ›´æ–°è·¯å¾„ä»¥åŒ…å« 'macro_report'
        report_dir = os.path.join(self.project_root, 'macro_report')
        latest_report = self.load_latest_file(os.path.join(report_dir, "*.md"))
        
        if not latest_report:
            return "N/A", "N/A", "No macro report found in macro_report/"
            
        with open(latest_report, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # ç®€å•è§£æ Risk Signal å’Œ Volatility (å‡è®¾æ ¼å¼å›ºå®š)
        risk = "Risk-Off" if "Risk-Off" in content else "Risk-On"
        vol = "Unknown" # å¯ä»¥åŠ æ­£åˆ™æå– 17.xx%
        
        # æˆªå–å‰ 1000 å­—ä½œä¸ºæ‘˜è¦
        summary = content[:1000] 
        return risk, vol, summary

    def _load_cache(self):
        """åŠ è½½æœ¬åœ°ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶ï¼ˆåŸå­å†™å…¥ï¼Œé¿å…æ•°æ®æŸåï¼‰"""
        with self.cache_lock:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡ŒåŸå­å†™å…¥
            import tempfile
            temp_dir = os.path.dirname(self.cache_file)
            with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', 
                                             dir=temp_dir, delete=False) as f:
                json.dump(self.metadata_cache, f, ensure_ascii=False)
                temp_path = f.name
            # åŸå­æ›¿æ¢
            import shutil
            shutil.move(temp_path, self.cache_file)
            
    def _save_snapshot(self):
        """ä¿å­˜æœ¬æ¬¡åˆ†æä¸­æ–°è·å–çš„å…¬å¸æ•°æ®å¿«ç…§"""
        if not self.newly_fetched_tickers:
            self.log("â„¹ï¸ æœ¬æ¬¡åˆ†ææ²¡æœ‰è·å–åˆ°æ–°çš„å…¬å¸æ•°æ®")
            return
        
        # æå–æœ¬æ¬¡è·å–çš„æ•°æ®
        snapshot_data = {}
        for ticker in self.newly_fetched_tickers:
            if ticker in self.metadata_cache:
                snapshot_data[ticker] = self.metadata_cache[ticker]
        
        if not snapshot_data:
            return
        
        # åˆ›å»ºå¿«ç…§æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        snapshot_dir = os.path.join(self.project_root, 'data', 'company_snapshots')
        os.makedirs(snapshot_dir, exist_ok=True)
        snapshot_file = os.path.join(snapshot_dir, f'company_data_{timestamp}.json')
        
        with open(snapshot_file, 'w', encoding='utf-8') as f:
            json.dump(snapshot_data, f, ensure_ascii=False, indent=2)
        
        self.log(f"ğŸ’¾ æœ¬æ¬¡åˆ†æçš„å…¬å¸æ•°æ®å¿«ç…§å·²ä¿å­˜: {snapshot_file}")
        return snapshot_file

    def get_stock_metadata(self, ticker, record_new=True):
        """è·å–è‚¡ç¥¨åç§°å’Œè¡Œä¸šä¿¡æ¯ (æ¶ˆé™¤å¹»è§‰çš„å…³é”®)"""
        # 1. å…ˆæŸ¥ç¼“å­˜
        with self.cache_lock:
            if ticker in self.metadata_cache:
                return self.metadata_cache[ticker]['name'], self.metadata_cache[ticker]['sector']

        # 2. æ ¼å¼è½¬æ¢ (.SH -> .SS ç”¨äº yfinance)
        yf_ticker = ticker.replace(".SH", ".SS") if ".SH" in ticker else ticker
        
        # é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                info = yf.Ticker(yf_ticker).info
                # ä¼˜å…ˆå–ä¸­æ–‡åæˆ–ç®€ç§°ï¼ŒYahoo Aè‚¡é€šå¸¸æ˜¯è‹±æ–‡åï¼ŒAIèƒ½ç¿»è¯‘
                name = info.get('shortName', info.get('longName', 'Unknown'))
                sector = info.get('sector', info.get('industry', 'Unknown'))
                
                # å¦‚æœè·å–åˆ°çš„ä¿¡æ¯ä¸ºç©ºï¼Œå¯èƒ½æ˜¯è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•
                if not info:
                    self.log(f"âš ï¸  è·å– {ticker} ä¿¡æ¯ä¸ºç©ºï¼Œé‡è¯• {attempt+1}/{max_retries}")
                    continue
                
                # 3. å†™å…¥ç¼“å­˜ï¼ˆåªæœ‰å½“æ•°æ®æœ‰æ•ˆæ—¶ï¼‰
                if name != 'Unknown':
                    with self.cache_lock:
                        self.metadata_cache[ticker] = {'name': name, 'sector': sector}
                        self._save_cache()
                        # è®°å½•æ–°è·å–çš„è‚¡ç¥¨ä»£ç 
                        if record_new:
                            self.newly_fetched_tickers.add(ticker)
                    
                return name, sector
            except Exception as e:
                self.log(f"âš ï¸  è·å– {ticker} å…ƒæ•°æ®å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                else:
                    return "Unknown", "Unknown"
        return "Unknown", "Unknown"

    def load_momentum_data(self, market_type):
        """è¯»å– CSV å¹¶æ³¨å…¥å…ƒæ•°æ®"""
        # æ›´æ–°è·¯å¾„ä»¥åŒ…å« 'micro_report'
        csv_dir = os.path.join(self.project_root, 'micro_report')
        pattern = f"Top200_Momentum_{market_type}_*.csv"
        latest_csv = self.load_latest_file(os.path.join(csv_dir, pattern))
        
        if not latest_csv:
            print(f"âš ï¸ No CSV found for {market_type} in {csv_dir}")
            return "No data"
            
        df = pd.read_csv(latest_csv)
        # å–å‰ 50 åï¼Œé¿å… Token æº¢å‡ºï¼Œä¸”å¤´éƒ¨æ•ˆåº”æœ€æ˜æ˜¾
        top_50 = df.head(50) 
        
        print(f"ğŸ” æ­£åœ¨è”ç½‘æ ¡å‡† {market_type} å‰ 50 åè‚¡ç¥¨çš„ä¸šåŠ¡ä¿¡æ¯...")
        stock_list_str = []
        
        # å¹¶å‘è·å–ï¼Œé¿å…å¡é¡¿
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_ticker = {executor.submit(self.get_stock_metadata, row['ticker']): row for _, row in top_50.iterrows()}
            
            for future in concurrent.futures.as_completed(future_to_ticker):
                row = future_to_ticker[future]
                name, sector = future.result()
                
                # [MODIFIED] è°ƒç”¨ç»Ÿä¸€çš„æ ¼å¼åŒ–å‡½æ•°
                line = self._format_stock_line(row, name, sector)
                stock_list_str.append(line)
        
        return "\n".join(stock_list_str)

    def run_analysis(self, macro_path=None, cn_path=None, us_path=None):
        self.log("ğŸš€ å¯åŠ¨è¡Œä¸šè¶‹åŠ¿åˆ†æå¼•æ“...")
        
        # æ¸…ç©ºæœ¬æ¬¡åˆ†æçš„æ–°è·å–è‚¡ç¥¨è®°å½•
        self.newly_fetched_tickers.clear()
        
        # 1. å‡†å¤‡æ•°æ®
        if macro_path and os.path.exists(macro_path):
            with open(macro_path, 'r', encoding='utf-8') as f:
                content = f.read()
            risk = "Risk-Off" if "Risk-Off" in content else "Risk-On"
            vol = "Unknown"
            macro_summary = content[:1000]
        else:
            risk, vol, macro_summary = self.load_macro_context()
            
        # Micro CN & US
        if cn_path and os.path.exists(cn_path):
            cn_stocks = self._process_csv(cn_path, 'CN')
        else:
            cn_stocks = self.load_momentum_data('CN')

        if us_path and os.path.exists(us_path):
            us_stocks = self._process_csv(us_path, 'US')
        else:
            us_stocks = self.load_momentum_data('US')
        
        if cn_stocks == "No data" and us_stocks == "No data":
            self.log("âŒ ç¼ºå°‘åŠ¨é‡æ•°æ®ï¼Œæ— æ³•åˆ†æ")
            return None, None

        # 2. æ„å»º Prompt (æ–°å¢æœ€åä¸€æ®µ Metadata æŒ‡ä»¤)
        prompt = f"""
# Role
ä½ æ˜¯ä¸€ä½ç²¾é€šå…¨çƒäº§ä¸šé“¾çš„èµ„æ·±é‡åŒ–ç­–ç•¥åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæˆ‘æä¾›çš„"å®è§‚ç¯å¢ƒ"å’Œ"å¸‚åœºå¼ºåŠ¿è‚¡æ¸…å•"ï¼Œé€šè¿‡å½’çº³æ³•æ¨å¯¼å‡ºå½“å‰å¤„äº"æ™¯æ°”åº¦ä¸Šè¡ŒåŒºé—´"çš„è¡Œä¸šæ¿å—ã€‚

# Input Data
## 1. Macro Context (å®è§‚èƒŒæ™¯)
- **Market Status**: {risk} (Risk-On / Risk-Off)
- **Volatility**: {vol}
- **Key Trend**: {macro_summary}

## 2. Micro Evidence (å¾®è§‚èµ„é‡‘æµå‘)
**æŒ‡æ ‡è¯´æ˜**:
- **æ¶¨å¹…**: è¿‡å» 60 æ—¥çš„ä»·æ ¼å˜åŒ–ã€‚
- **RSRS (Trend Strength)**: è¶‹åŠ¿ç»“æ„å¼ºåº¦æŒ‡æ ‡ (èŒƒå›´ -1.0 ~ 1.0)ã€‚
    - **> 0.8 (ğŸ”¥)**: å¼ºåŠ²çš„å¤šå¤´è¶‹åŠ¿ç»“æ„ï¼ˆé˜»åŠ›è¢«çªç ´ï¼Œæ”¯æ’‘å¼ºåŠ²ï¼‰ï¼Œä»£è¡¨èµ„é‡‘æŒç»­æµå…¥ï¼Œ**è¡Œä¸šé€»è¾‘çœŸå®æ€§é«˜**ã€‚
    - **< 0.3**: è¶‹åŠ¿ç»“æ„æ¾æ•£æˆ–å¤„äºéœ‡è¡ï¼Œå•çº¯çš„æ¶¨å¹…å¯èƒ½æ¥è‡ªçŸ­æœŸæ¶ˆæ¯ç‚’ä½œã€‚

**[A-Share Top Momentum]**
{cn_stocks} 

**[US-Share Top Momentum]**
{us_stocks}

# Analysis Requirements
1.  **è¡Œä¸šæ˜ å°„**ï¼šè¯†åˆ«è‚¡ç¥¨ä»£ç å¯¹åº”çš„ç»†åˆ†èµ›é“ã€‚
2.  **é›†ç¾¤è¯†åˆ«**ï¼šæ‰¾å‡ºå‡ºç°é¢‘æ¬¡æœ€é«˜çš„ 3-5 ä¸ªç»†åˆ†è¡Œä¸šã€‚
3.  **é‡åŒ–éªŒè¯ (Critical)**ï¼š
    - **ä¸ä»…ä»…çœ‹æ¶¨å¹…ï¼Œæ›´è¦çœ‹ RSRS**ã€‚
    - ä¼˜å…ˆç­›é€‰å‡ºé‚£äº›**æ¶¨å¹…é«˜ä¸” RSRS > 0.8** çš„è‚¡ç¥¨æ‰€åœ¨çš„æ¿å—ã€‚è¿™ä»£è¡¨è¯¥æ¿å—ä¸ä»…æ¶¨äº†ï¼Œè€Œä¸”æ¶¨å¾—å¾ˆç¨³ï¼ˆè¶‹åŠ¿ç»“æ„å¥½ï¼‰ï¼Œæ˜¯æœºæ„èµ„é‡‘æŠ±å›¢çš„ç‰¹å¾ã€‚
    - å¦‚æœæŸè¡Œä¸šè‚¡ç¥¨æ¶¨å¹…å¤§ä½† RSRS æ™®éè¾ƒä½ï¼Œè¯·åœ¨æŠ¥å‘Šä¸­æ ‡è®°ä¸º"æŠ•æœºæ€§ä¸Šæ¶¨"ã€‚
4.  **å®è§‚éªŒè¯**ï¼šç»“åˆå®è§‚èƒŒæ™¯åˆ†æåˆç†æ€§ã€‚

# Output Format
è¯·ç”Ÿæˆä¸€ä»½ Markdown æ ¼å¼çš„ã€Šè¡Œä¸šæ™¯æ°”åº¦æ·±åº¦æ‰«ææŠ¥å‘Šã€‹ï¼ŒåŒ…å«ï¼š
1.  **æ ¸å¿ƒç»“è®º** (å¿…é¡»åŒ…å«å¯¹ RSRS ç¡®è®¤å¼ºåº¦çš„æè¿°)
2.  **æ™¯æ°”åº¦æ’è¡Œ** (åˆ—å‡ºæœ€å¼ºè¡Œä¸šï¼Œå¹¶æ³¨æ˜å…¶"è¶‹åŠ¿å¼ºåº¦ç­‰çº§")
3.  **äº§ä¸šé“¾æ˜ å°„å›¾è°±** (å…±æŒ¯é€»è¾‘)
4.  **é£é™©æç¤º**

# ğŸ›‘ IMPORTANT: Metadata Output
TITLE: [ä½ çš„æ ‡é¢˜]
"""
        
        # 3. è°ƒç”¨ API
        self.log("ğŸ§  æ­£åœ¨è°ƒç”¨ DeepSeek è¿›è¡Œäº§ä¸šé“¾èšç±»åˆ†æ...")
        try:
            response = self.strategist.client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": "You are a professional financial analyst."},
                    {"role": "user", "content": prompt}
                ],
                stream=False
            )
            
            raw_content = response.choices[0].message.content
            
            # ç¡®ä¿ raw_content ä¸æ˜¯ None
            if raw_content is None:
                raw_content = ""
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šæå–è¯­ä¹‰åŒ–æ ‡é¢˜ ---
            import re
            title_match = re.search(r"TITLE:\s*(.*)", raw_content)
            
            if title_match:
                # æå–æ ‡é¢˜
                semantic_title = title_match.group(1).strip()
                # ä»æ­£æ–‡ä¸­ç§»é™¤ TITLE: è¡Œï¼Œä¿æŒæŠ¥å‘Šæ•´æ´
                report_content = raw_content.replace(title_match.group(0), "").strip()
            else:
                # Fallback: å¦‚æœæ²¡ç”Ÿæˆï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')
                semantic_title = f"è¡Œä¸šå…¨æ™¯æ‰«æ ({timestamp})"
                report_content = raw_content
            
            # ç¡®ä¿ report_content ä¸æ˜¯ None
            if report_content is None:
                report_content = ""

            # --- ä¿å­˜æ–‡ä»¶ (ä¿æŒæ—¶é—´æˆ³æ–‡ä»¶åï¼Œä¾¿äºæ’åº) ---
            model_name = self.config.model.replace("/", "-") if self.config.model else "unknown"
            timestamp_file = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            filename = f"report_by_{model_name}_{timestamp_file}.md"
            
            save_path = os.path.join(self.project_root, 'research_report', filename)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
                
            self.log(f"âœ… è¡Œä¸šåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
            
            # --- ä¿å­˜å…¬å¸æ•°æ®å¿«ç…§ ---
            self._save_snapshot()
            
            # --- å­˜å…¥æ•°æ®åº“ (ä½¿ç”¨è¯­ä¹‰åŒ–æ ‡é¢˜) ---
            self.log(f"ğŸ’¾ æ­£åœ¨è‡ªåŠ¨å½’æ¡£: ã€Š{semantic_title}ã€‹")
            
            current_analyst = self.config.model if self.config.model else "deepseek-chat"
            
            save_research_report(
                title=semantic_title,  # <--- è¿™é‡Œå­˜å…¥è¯­ä¹‰åŒ–æ ‡é¢˜
                content=report_content, 
                tags="Industry, DeepSeek",
                analyst=current_analyst
            )
            
            return report_content, filename
            
        except Exception as e:
            self.log(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            # æ‰“å°è¯¦ç»†å †æ ˆä»¥ä¾¿è°ƒè¯•
            import traceback
            traceback.print_exc()
            return None, None

if __name__ == "__main__":
    analyzer = IndustryAnalyzer()
    analyzer.run_analysis()
